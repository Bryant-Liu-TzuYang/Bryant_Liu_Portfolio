{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d9d5c-fb0f-4aed-8437-17f7c4798034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === 資料增強方法 ===\n",
    "def add_noise(signal, noise_level=0.005):\n",
    "    noise = np.random.randn(len(signal))\n",
    "    return signal + noise_level * noise\n",
    "\n",
    "def random_gain(signal, min_gain=0.8, max_gain=1.2):\n",
    "    gain = np.random.uniform(min_gain, max_gain)\n",
    "    return signal * gain\n",
    "\n",
    "def safe_time_warp(signal, rate_range=(0.8, 1.2), target_len=44100):\n",
    "    try:\n",
    "        rate = np.random.uniform(rate_range[0], rate_range[1])\n",
    "        warped = librosa.effects.time_stretch(signal, rate=rate)\n",
    "        if len(warped) > target_len:\n",
    "            warped = warped[:target_len]\n",
    "        else:\n",
    "            warped = np.pad(warped, (0, target_len - len(warped)), mode='constant')\n",
    "        return warped\n",
    "    except Exception as e:\n",
    "        print(f\"time_warp 失敗：{e}\")\n",
    "        return signal\n",
    "\n",
    "# === 特徵提取方法 ===\n",
    "def extract_features(signal, sr=22050, method='logmel', delta=False, zscore=False):\n",
    "    if method == 'logmel':\n",
    "        feat = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=128)\n",
    "        feat = np.log(feat + 1e-6)\n",
    "        feat = librosa.util.fix_length(feat, size=256, axis=1)\n",
    "    elif method == 'mfcc':\n",
    "        feat = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=20)\n",
    "    elif method == 'stft':\n",
    "        D = librosa.stft(signal, n_fft=1024, hop_length=512)\n",
    "        feat = np.abs(D)\n",
    "        feat = np.log1p(feat) \n",
    "        feat = librosa.util.fix_length(feat, size=256, axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"不支援的特徵提取方法\")\n",
    "\n",
    "    if delta:\n",
    "        delta1 = librosa.feature.delta(feat)\n",
    "        delta2 = librosa.feature.delta(feat, order=2)\n",
    "        feat = np.vstack([feat, delta1, delta2])\n",
    "\n",
    "    if zscore:\n",
    "        feat = np.clip(feat, -3.0, 3.0)\n",
    "        scaler = StandardScaler()\n",
    "        feat = scaler.fit_transform(feat.T).T\n",
    "\n",
    "    if feat.shape[0] > 128:\n",
    "        feat = feat[:128, :]\n",
    "    elif feat.shape[0] < 128:\n",
    "        pad_width = 128 - feat.shape[0]\n",
    "        feat = np.pad(feat, ((0, pad_width), (0, 0)), mode='constant')\n",
    "\n",
    "    return feat[..., np.newaxis]\n",
    "\n",
    "# === 蒐集資料檔案路徑 ===\n",
    "def get_data_sources(base_dir=\"../dataset\"):\n",
    "    data_sources = {}\n",
    "    data_sources['normal'] = os.path.join(base_dir, 'normal/normal')\n",
    "\n",
    "    for fault_type in ['imbalance', 'horizontal-misalignment', 'vertical-misalignment']:\n",
    "        fault_path = os.path.join(base_dir, fault_type, fault_type)\n",
    "        for folder in os.listdir(fault_path):\n",
    "            data_sources[fault_type] = os.path.join(fault_path, folder)\n",
    "            break\n",
    "\n",
    "    for align in ['overhang', 'underhang']:\n",
    "        align_path = os.path.join(base_dir, align, align)\n",
    "        for fault_type in os.listdir(align_path):\n",
    "            fault_dir = os.path.join(align_path, fault_type)\n",
    "            label = f\"{align}_{fault_type}\"\n",
    "            for weight in os.listdir(fault_dir):\n",
    "                data_sources[label] = os.path.join(fault_dir, weight)\n",
    "\n",
    "    return data_sources\n",
    "\n",
    "# === 收集檔案路徑並切分為 train/test 檔案清單 ===\n",
    "def split_file_paths(data_sources, test_size=0.2, random_state=42):\n",
    "    train_files = []\n",
    "    test_files = []\n",
    "    label_map = {label: idx for idx, label in enumerate(data_sources.keys())}\n",
    "\n",
    "    for label, root_path in data_sources.items():\n",
    "        files = []\n",
    "        for dirpath, _, filenames in os.walk(root_path):\n",
    "            for file in filenames:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    files.append(os.path.join(dirpath, file))\n",
    "        train, test = train_test_split(files, test_size=test_size, random_state=random_state)\n",
    "        train_files.extend([(f, label_map[label]) for f in train])\n",
    "        test_files.extend([(f, label_map[label]) for f in test])\n",
    "\n",
    "    return train_files, test_files, label_map\n",
    "\n",
    "# === 根據檔案清單收集音訊片段資料 ===\n",
    "def collect_segments(file_label_pairs, segment_len=44100, sr=22050, augment=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    for file_path, label in file_label_pairs:\n",
    "        try:\n",
    "            data = pd.read_csv(file_path, header=None)\n",
    "            signal = data.iloc[:, 7].values.astype(np.float32)\n",
    "            for start in range(0, len(signal) - segment_len + 1, segment_len):\n",
    "                segment = signal[start:start + segment_len]\n",
    "                variants = [segment]\n",
    "                if augment:\n",
    "                    variants += [\n",
    "                        add_noise(segment),\n",
    "                        random_gain(segment),\n",
    "                        safe_time_warp(segment)\n",
    "                    ]\n",
    "                for sig in variants:\n",
    "                    X.append(sig)\n",
    "                    y.append(label)\n",
    "\n",
    "            # 補尾巴\n",
    "            remainder = len(signal) % segment_len\n",
    "            if remainder != 0:\n",
    "                last_start = len(signal) - remainder\n",
    "                segment = np.pad(signal[last_start:], (0, segment_len - remainder), mode='constant')\n",
    "                variants = [segment]\n",
    "                if augment:\n",
    "                    variants += [\n",
    "                        add_noise(segment),\n",
    "                        random_gain(segment),\n",
    "                        safe_time_warp(segment)\n",
    "                    ]\n",
    "                for sig in variants:\n",
    "                    X.append(sig)\n",
    "                    y.append(label)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"錯誤：{file_path} - {e}\")\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# === 主程式 ===\n",
    "data_sources = get_data_sources()\n",
    "train_files, test_files, label_map = split_file_paths(data_sources)\n",
    "X_train_raw, y_train = collect_segments(train_files, augment=True)\n",
    "X_test_raw, y_test = collect_segments(test_files, augment=False)\n",
    "\n",
    "# === 特徵提取 ===\n",
    "X_train_feat = np.array([\n",
    "    extract_features(x, method='logmel', delta=True, zscore=True)\n",
    "    for x in X_train_raw\n",
    "])\n",
    "X_test_feat = np.array([\n",
    "    extract_features(x, method='logmel', delta=True, zscore=True)\n",
    "    for x in X_test_raw\n",
    "])\n",
    "\n",
    "# === 儲存 ===\n",
    "np.savez_compressed(\"train_data.npz\", X=X_train_feat, y=y_train)\n",
    "np.savez_compressed(\"test_data.npz\", X=X_test_feat, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6edd4d-fc11-4b11-aed9-ad0547dac120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
