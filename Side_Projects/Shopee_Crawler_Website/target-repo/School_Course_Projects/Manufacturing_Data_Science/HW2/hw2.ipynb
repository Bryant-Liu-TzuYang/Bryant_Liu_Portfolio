{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1118bf1",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9ee29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/bryant_lue/Downloads/113-2-data-mining-homework-2\"\n",
    "\n",
    "df_train = pd.read_json(f\"{file_path}/train_data.json\")\n",
    "df_test = pd.read_json(f\"{file_path}/test_data.json\")\n",
    "df_train_label = pd.read_csv(f\"{file_path}/train_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8584d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(df_train_label, on=\"Pid\")\n",
    "df_train[\"Postdate\"] = pd.to_datetime(df_train[\"Postdate\"], unit='s')\n",
    "df_test[\"Postdate\"] = pd.to_datetime(df_test[\"Postdate\"], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba5a72fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pid</th>\n",
       "      <th>Uid</th>\n",
       "      <th>Title</th>\n",
       "      <th>Alltags</th>\n",
       "      <th>Category</th>\n",
       "      <th>Concept</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Postdate</th>\n",
       "      <th>img_filepath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149005</td>\n",
       "      <td>22687@N84</td>\n",
       "      <td>having a drink</td>\n",
       "      <td>life county wild bird water animal closeup fau...</td>\n",
       "      <td>Food</td>\n",
       "      <td>thirsty</td>\n",
       "      <td>Drinks</td>\n",
       "      <td>2015-03-13 03:21:30</td>\n",
       "      <td>train/22687@N84/149005.jpg</td>\n",
       "      <td>10.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149948</td>\n",
       "      <td>17614@N19</td>\n",
       "      <td>Foto Agne Sterberg, Destination Hga Kusten, AG...</td>\n",
       "      <td>hav mitt hga kusten blsippor nordingr klippor ...</td>\n",
       "      <td>Travel&amp;Active&amp;Sports</td>\n",
       "      <td>mitt</td>\n",
       "      <td>Baseball</td>\n",
       "      <td>2015-03-17 02:05:20</td>\n",
       "      <td>train/17614@N19/149948.jpg</td>\n",
       "      <td>6.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151388</td>\n",
       "      <td>17614@N19</td>\n",
       "      <td>Foto Agne Sterberg, AGMA Forntid &amp; ventyr AB, ...</td>\n",
       "      <td>is sweden sverige hav soluppgng mitt vr hga ku...</td>\n",
       "      <td>Travel&amp;Active&amp;Sports</td>\n",
       "      <td>mitt</td>\n",
       "      <td>Baseball</td>\n",
       "      <td>2015-03-24 21:29:17</td>\n",
       "      <td>train/17614@N19/151388.jpg</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151389</td>\n",
       "      <td>17614@N19</td>\n",
       "      <td>Foto Agne Sterberg, AGMA Forntid &amp; ventyr AB, ...</td>\n",
       "      <td>is sweden sverige hav soluppgng mitt vr hga ku...</td>\n",
       "      <td>Travel&amp;Active&amp;Sports</td>\n",
       "      <td>mitt</td>\n",
       "      <td>Baseball</td>\n",
       "      <td>2015-03-24 10:18:36</td>\n",
       "      <td>train/17614@N19/151389.jpg</td>\n",
       "      <td>5.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151390</td>\n",
       "      <td>17614@N19</td>\n",
       "      <td>Foto Agne Sterberg, AGMA Forntid &amp; ventyr AB, ...</td>\n",
       "      <td>is sweden sverige hav soluppgng mitt vr hga ku...</td>\n",
       "      <td>Travel&amp;Active&amp;Sports</td>\n",
       "      <td>mitt</td>\n",
       "      <td>Baseball</td>\n",
       "      <td>2015-03-24 21:55:46</td>\n",
       "      <td>train/17614@N19/151390.jpg</td>\n",
       "      <td>5.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pid        Uid                                              Title  \\\n",
       "0  149005  22687@N84                                     having a drink   \n",
       "1  149948  17614@N19  Foto Agne Sterberg, Destination Hga Kusten, AG...   \n",
       "2  151388  17614@N19  Foto Agne Sterberg, AGMA Forntid & ventyr AB, ...   \n",
       "3  151389  17614@N19  Foto Agne Sterberg, AGMA Forntid & ventyr AB, ...   \n",
       "4  151390  17614@N19  Foto Agne Sterberg, AGMA Forntid & ventyr AB, ...   \n",
       "\n",
       "                                             Alltags              Category  \\\n",
       "0  life county wild bird water animal closeup fau...                  Food   \n",
       "1  hav mitt hga kusten blsippor nordingr klippor ...  Travel&Active&Sports   \n",
       "2  is sweden sverige hav soluppgng mitt vr hga ku...  Travel&Active&Sports   \n",
       "3  is sweden sverige hav soluppgng mitt vr hga ku...  Travel&Active&Sports   \n",
       "4  is sweden sverige hav soluppgng mitt vr hga ku...  Travel&Active&Sports   \n",
       "\n",
       "   Concept Subcategory            Postdate                img_filepath  label  \n",
       "0  thirsty      Drinks 2015-03-13 03:21:30  train/22687@N84/149005.jpg  10.07  \n",
       "1     mitt    Baseball 2015-03-17 02:05:20  train/17614@N19/149948.jpg   6.27  \n",
       "2     mitt    Baseball 2015-03-24 21:29:17  train/17614@N19/151388.jpg   5.46  \n",
       "3     mitt    Baseball 2015-03-24 10:18:36  train/17614@N19/151389.jpg   5.39  \n",
       "4     mitt    Baseball 2015-03-24 21:55:46  train/17614@N19/151390.jpg   5.36  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41c0e5",
   "metadata": {},
   "source": [
    "### EDA\n",
    "- train Category.unique == test Category.unique?\n",
    "- train Concept.unique == test Concept.unique?\n",
    "- train Category.unique == test Category.unique?\n",
    "\n",
    "- count\n",
    "- 直接找差集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0f2a85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Category, dtype: object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[~df_train['Category'].isin(df_test['Category'])]['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "775d9b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Category, dtype: object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[~df_test['Category'].isin(df_train['Category'])]['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "752b28e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26          skywatcher\n",
       "28          skywatcher\n",
       "29          skywatcher\n",
       "30              repeat\n",
       "31            tumbling\n",
       "             ...      \n",
       "14988          blessed\n",
       "14990    unforgettable\n",
       "14993             fans\n",
       "14997           repeat\n",
       "14999    modifications\n",
       "Name: Concept, Length: 7674, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[~df_train['Concept'].isin(df_test['Concept'])]['Concept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9a595dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146      cocktails\n",
       "147      cocktails\n",
       "302         jacket\n",
       "382      motorbike\n",
       "511      motorbike\n",
       "           ...    \n",
       "4755    gymnastics\n",
       "4872      swimming\n",
       "4894     earlybird\n",
       "4897     earlybird\n",
       "4925          shoe\n",
       "Name: Concept, Length: 694, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[~df_test['Concept'].isin(df_train['Concept'])]['Concept']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f924eaf",
   "metadata": {},
   "source": [
    "### Extract image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a9cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using M1 Pro GPU via MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [08:43<00:00, 28.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features extracted for training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:56<00:00, 28.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features extracted for test data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "# set path\n",
    "os.chdir(file_path)\n",
    "\n",
    "# Load ResNet model\n",
    "resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "resnet.fc = torch.nn.Identity()\n",
    "resnet.eval()\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using M1 Pro GPU via MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available; using CPU\")\n",
    "resnet.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(img_paths):\n",
    "    features = []\n",
    "    for path in tqdm(img_paths):\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            img = transform(img).unsqueeze(0)\n",
    "            img = img.to(device)\n",
    "            with torch.no_grad():\n",
    "                feature = resnet(img).squeeze().cpu().numpy()\n",
    "            features.append(feature)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with image {path}: {e}\")\n",
    "            features.append(np.zeros(2048))\n",
    "    return np.array(features)\n",
    "\n",
    "image_features_train = extract_features(df_train['img_filepath'])\n",
    "print(\"Image features extracted for training data.\")\n",
    "image_features_test = extract_features(df_test['img_filepath'])\n",
    "print(\"Image features extracted for test data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13df76",
   "metadata": {},
   "source": [
    "### Feature Engineering: Concating image featruesm TFIDF and onehotencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a05674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 2048)\n",
      "(5000, 2048)\n",
      "(15000, 2878)\n",
      "(5000, 2878)\n",
      "(15000, 2879)\n",
      "(5000, 2879)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Convert image features to DataFrame\n",
    "df_image_features_train = pd.DataFrame(image_features_train, index=df_train.index)\n",
    "df_image_features_train.columns = [f\"img_feat_{i}\" for i in range(image_features_train.shape[1])]\n",
    "print(image_features_train.shape)\n",
    "\n",
    "df_image_features_test = pd.DataFrame(image_features_test, index=df_test.index)\n",
    "df_image_features_test.columns = [f\"img_feat_{i}\" for i in range(image_features_test.shape[1])]\n",
    "print(image_features_test.shape)\n",
    "\n",
    "# Define Concatenate DataFrames\n",
    "df_train_concat = pd.concat([df_train, df_image_features_train], axis=1)\n",
    "df_test_concat = pd.concat([df_test, df_image_features_test], axis=1)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# TF-IDF Vectorization\n",
    "# Combine relevant text columns\n",
    "df_train['text'] = df_train['Title'].astype(str) + \" \" + df_train['Alltags'].astype(str)\n",
    "df_test['text'] = df_test['Title'].astype(str) + \" \" + df_test['Alltags'].astype(str)\n",
    "\n",
    "# TF-IDF Vectorizer for the combined text\n",
    "tfidf = TfidfVectorizer(max_features=100)  # You can adjust max_features\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix_train = tfidf.fit_transform(df_train['text']).toarray()\n",
    "tfidf_matrix_test = tfidf.fit_transform(df_test['text']).toarray()\n",
    "\n",
    "df_ifidf_train = pd.DataFrame(tfidf_matrix_train, index=df_train.index)\n",
    "df_ifidf_test = pd.DataFrame(tfidf_matrix_test, index=df_test.index)\n",
    "\n",
    "# Optionally, rename columns\n",
    "df_ifidf_train.columns = [f\"tfidf_feat_{i}\" for i in range(tfidf_matrix_train.shape[1])]\n",
    "df_ifidf_test.columns = [f\"tfidf_feat_{i}\" for i in range(tfidf_matrix_test.shape[1])]\n",
    "df_train_concat = pd.concat([df_train_concat, df_ifidf_train], axis=1)\n",
    "df_test_concat = pd.concat([df_test_concat, df_ifidf_test], axis=1)\n",
    "\n",
    "\n",
    "# You can also vectorize 'Category' (if categorical, use OneHotEncoder instead)\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "category_encoded_train = ohe.fit_transform(df_train[['Category']])\n",
    "df_category_train = pd.DataFrame(category_encoded_train, index=df_train.index)\n",
    "df_category_train.columns = ohe.categories_[0]\n",
    "\n",
    "category_encoded_test = ohe.fit_transform(df_test[['Category']])\n",
    "df_category_test = pd.DataFrame(category_encoded_test, index=df_test.index)\n",
    "df_category_test.columns = ohe.categories_[0]\n",
    "\n",
    "# Concept\n",
    "concept_encoded_train = ohe.fit_transform(df_train[['Concept']])\n",
    "df_concept_train = pd.DataFrame(concept_encoded_train, index=df_train.index)\n",
    "df_concept_train.columns = ohe.get_feature_names_out(['Concept'])\n",
    "\n",
    "concept_encoded_test = ohe.transform(df_test[['Concept']])\n",
    "df_concept_test = pd.DataFrame(concept_encoded_test, index=df_test.index)\n",
    "df_concept_test.columns = ohe.get_feature_names_out(['Concept'])\n",
    "\n",
    "# Align test columns to match train columns, fill missing ones with 0\n",
    "df_concept_test = df_concept_test.reindex(columns=df_concept_train.columns, fill_value=0)\n",
    "\n",
    "# Subcategory\n",
    "subcat_encoded_train = ohe.fit_transform(df_train[['Subcategory']])\n",
    "df_subcat_train = pd.DataFrame(subcat_encoded_train, index=df_train.index)\n",
    "df_subcat_train.columns = ohe.get_feature_names_out(['Subcategory'])\n",
    "\n",
    "subcat_encoded_test = ohe.transform(df_test[['Subcategory']])\n",
    "df_subcat_test = pd.DataFrame(subcat_encoded_test, index=df_test.index)\n",
    "df_subcat_test.columns = ohe.get_feature_names_out(['Subcategory'])\n",
    "\n",
    "df_subcat_test = df_subcat_test.reindex(columns=df_subcat_train.columns, fill_value=0)\n",
    "\n",
    "\n",
    "# Combine all features (text, categorical, etc.)\n",
    "df_train_concat = pd.concat([df_train_concat, df_category_train, df_concept_train, df_subcat_train], axis=1)\n",
    "df_test_concat = pd.concat([df_test_concat, df_category_test, df_concept_test, df_subcat_test], axis=1)\n",
    "\n",
    "print(df_train_concat.shape)\n",
    "print(df_test_concat.shape)\n",
    "\n",
    "tfidf_category_features_train = np.hstack([tfidf_matrix_train, category_encoded_train, concept_encoded_train, subcat_encoded_train])\n",
    "tfidf_category_features_test = np.hstack([tfidf_matrix_test, category_encoded_test, concept_encoded_test, subcat_encoded_test])\n",
    "\n",
    "# Additional simple features\n",
    "df_train['Posthour'] = pd.to_datetime(df_train['Postdate']).dt.hour\n",
    "df_test['Posthour'] = pd.to_datetime(df_test['Postdate']).dt.hour\n",
    "text_features_train = pd.get_dummies(df_train[['Posthour']], drop_first=True).values\n",
    "text_features_test = pd.get_dummies(df_test[['Posthour']], drop_first=True).values\n",
    "\n",
    "# Combine features\n",
    "X = np.concatenate((image_features_train, text_features_train, tfidf_category_features_train), axis=1)\n",
    "y = df_train['label']\n",
    "\n",
    "\n",
    "\n",
    "df_hour_train = pd.DataFrame(text_features_train, index=df_train.index)\n",
    "df_hour_test = pd.DataFrame(text_features_test, index=df_test.index)\n",
    "\n",
    "\n",
    "df_train_concat = pd.concat([df_train_concat, df_hour_train], axis=1)\n",
    "df_test_concat = pd.concat([df_test_concat, df_hour_test], axis=1)\n",
    "print(df_train_concat.shape)\n",
    "print(df_test_concat.shape)\n",
    "\n",
    "df_train_concat.to_csv(\"train_concat.csv\", index=False) \n",
    "df_test_concat.to_csv(\"test_concat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971bea3",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fa8f7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.6532830666666665\n",
      "R²: 0.2895075785925194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print('MAE:', mean_absolute_error(y_test, y_pred))\n",
    "print('R²:', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ebc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "X_test_final = np.concatenate((image_features_test, text_features_test, tfidf_category_features_test), axis=1)\n",
    "y_test_pred = model.predict(X_test_final)\n",
    "df_test['Predicted'] = y_test_pred\n",
    "df_test[['Pid', 'Predicted']].to_csv(\"submission_random.csv\", index=False)\n",
    "print(\"Predictions saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fdba25",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4fff03ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DS311/lib/python3.11/site-packages/xgboost/callback.py:386: UserWarning: [17:57:33] WARNING: /Users/runner/work/xgboost/xgboost/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:1.97111\tvalidation-mae:2.01304\n",
      "[500]\ttrain-mae:1.30303\tvalidation-mae:1.65284\n",
      "[1000]\ttrain-mae:0.92448\tvalidation-mae:1.60689\n",
      "[1372]\ttrain-mae:0.74287\tvalidation-mae:1.60555\n",
      "RMSE: 2.124\n",
      "MAE: 1.606\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:absoluteerror',\n",
    "    'device': 'cuda',\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "evals = [(dtrain, 'train'), (dtest, 'validation')]\n",
    "\n",
    "model = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = dtrain,\n",
    "    num_boost_round = 10000,\n",
    "    evals = evals,\n",
    "    verbose_eval = 500,\n",
    "    early_stopping_rounds = 100,\n",
    ")\n",
    "\n",
    "preds = model.predict(dtest)\n",
    "rmse = root_mean_squared_error(y_test, preds)\n",
    "mae = mean_absolute_error(y_test, preds)\\\n",
    "\n",
    "print(f'RMSE: {rmse:.3f}')\n",
    "print(f'MAE: {mae:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e5bcb39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "X_test_final = np.concatenate((image_features_test, text_features_test, tfidf_category_features_test), axis=1)\n",
    "dtest = xgb.DMatrix(X_test_final)\n",
    "y_test_pred = model.predict(dtest)\n",
    "df_test['Predicted'] = y_test_pred\n",
    "df_test[['Pid', 'Predicted']].to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
